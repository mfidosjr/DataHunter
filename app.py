# app.py
import streamlit as st
import os
import pandas as pd
import zipfile
import plotly.express as px
import time

from search import buscar_paginas
from validation import validar_links_de_dados, extrair_tabelas_html
from downloader import baixar_arquivo
from analyzer import analisar_dataset
from api_detector import detectar_apis_na_pagina, baixar_dados_da_api
from semantic_analyzer import gerar_resumo_semantico

# --- Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Data Hunter 5.2 Alpha",
    page_icon="üîé",
    layout="wide"
)

st.title("üîé Data Hunter 5.2 Alpha")
st.write("Busca inteligente e an√°lise autom√°tica de datasets p√∫blicos!")

# --- Entrada do usu√°rio
consulta = st.text_input("Digite o tema da busca (ex: qualidade da √°gua no Brasil):")
buscar = st.button("Buscar e Analisar Dados")

if buscar and consulta:
    tempo_inicio = time.time()

    with st.spinner('üîç Buscando p√°ginas relevantes...'):
        paginas_encontradas = buscar_paginas(consulta)

    st.success(f"‚úÖ {len(paginas_encontradas)} p√°ginas encontradas.")
    st.divider()

    datasets_encontrados = []
    with st.spinner('üîé Vasculhando p√°ginas para encontrar datasets...'):
        for pagina in paginas_encontradas:
            links_validos = validar_links_de_dados(pagina)
            datasets_encontrados.extend(links_validos)

    datasets_encontrados = list(set(datasets_encontrados))
    st.success(f"‚úÖ {len(datasets_encontrados)} links de datasets encontrados.")
    st.divider()

    arquivos_baixados = []

    if datasets_encontrados:
        with st.spinner('üíæ Baixando datasets encontrados...'):
            for link in datasets_encontrados:
                caminho = baixar_arquivo(link)
                if caminho:
                    arquivos_baixados.append(caminho)
    else:
        st.info('‚ÑπÔ∏è Nenhum link direto encontrado. Tentando extrair tabelas HTML...')
        for idx_pag, pagina in enumerate(paginas_encontradas):
            tabelas = extrair_tabelas_html(pagina)
            for idx_tab, tabela in enumerate(tabelas):
                os.makedirs('datasets', exist_ok=True)
                caminho = f"datasets/pagina{idx_pag}_tabela{idx_tab}.csv"
                tabela.to_csv(caminho, index=False)
                arquivos_baixados.append(caminho)

        if not arquivos_baixados:
            st.info('‚ÑπÔ∏è Tentando detectar APIs JSON como fallback...')
            for pagina in paginas_encontradas:
                apis = detectar_apis_na_pagina(pagina)
                for idx, api_url in enumerate(apis):
                    caminho = baixar_dados_da_api(api_url, nome_base=f"api_extraida_{idx}")
                    if caminho:
                        arquivos_baixados.append(caminho)

    if not arquivos_baixados:
        st.error("‚ö†Ô∏è Nenhum dataset √∫til foi encontrado ap√≥s todos os m√©todos.")
    else:
        st.success(f"‚úÖ {len(arquivos_baixados)} datasets obtidos!")
        st.divider()

        resumos = []
        with st.spinner('üìä Analisando datasets...'):
            for caminho in arquivos_baixados:
                resumo = analisar_dataset(caminho)
                if resumo:
                    # An√°lise sem√¢ntica adicional
                    try:
                        df = pd.read_csv(caminho, encoding='utf-8', low_memory=False)
                        resumo['analise_semantica'] = gerar_resumo_semantico(df.columns)
                    except Exception as e:
                        resumo['analise_semantica'] = "Erro na leitura para an√°lise sem√¢ntica"
                    resumos.append(resumo)

        if resumos:
            df_resultados = pd.DataFrame(resumos).sort_values(by='pontuacao', ascending=False)
            st.success(f"‚úÖ {len(resumos)} datasets analisados com sucesso!")

            st.dataframe(df_resultados[['arquivo', 'linhas', 'colunas', '%_nulos', 'pontuacao', 'analise_semantica']])

            fig = px.bar(df_resultados, x='arquivo', y='pontuacao', color='pontuacao', color_continuous_scale=['red', 'orange', 'green'])
            st.plotly_chart(fig, use_container_width=True)

            zip_path = 'datasets_baixados.zip'
            with zipfile.ZipFile(zip_path, 'w') as zipf:
                for resumo in resumos:
                    zipf.write(resumo['caminho'], arcname=resumo['arquivo'])

            with open(zip_path, 'rb') as f:
                st.download_button("üì¶ Download de Todos os Datasets", f, file_name="datasets_baixados.zip")
        else:
            st.warning("‚ö†Ô∏è Nenhum dataset √∫til foi encontrado ap√≥s an√°lise.")

    tempo_fim = time.time()
    duracao = tempo_fim - tempo_inicio
    st.info(f"‚è±Ô∏è Tempo total de execu√ß√£o: {duracao:.2f} segundos.")

else:
    st.info("Digite um tema e clique no bot√£o para iniciar.")
